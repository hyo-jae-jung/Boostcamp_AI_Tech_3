{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053d1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff0f5c",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54939cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegression(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.wights = nn.Parameter(torch.randn(in_features,out_features))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        return x @ self.wights + self.bias        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102800a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MultiLinearRegression(7,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adcde3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0740,  3.8430, -0.0982, -2.5077, -1.8021,  2.5977, -4.3408,  1.4141,\n",
       "         -2.2272, -2.1032],\n",
       "        [-1.5730, -0.2255, -2.0736, -1.2394, -2.9800,  6.4943, -3.3821, -0.7074,\n",
       "         -1.6155,  1.7785],\n",
       "        [ 0.7693,  2.4711, -0.3356,  1.6714, -3.1446,  1.6486,  1.9221,  3.4830,\n",
       "         -5.3791, -2.1183],\n",
       "        [-0.5788,  3.3429,  3.1633, -0.1289,  2.5530, -0.5057,  0.0164,  3.8509,\n",
       "         -5.7834, -4.3207],\n",
       "        [-1.3865, -4.6907, -4.1598, -2.7589, -6.0391,  2.3497, -3.8719, -3.2154,\n",
       "          0.2382,  3.4618]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb951f4",
   "metadata": {},
   "source": [
    "**선형회귀분석 모듈이다.\n",
    " - 가중치 값들이 랜덤인 이유는 어차피 딥러닝을 시작하면 값을 찾아가기 때문에 초기값이라고 생각하면 된다.\n",
    " - 입력되는 데이터의 변수 개수에 맞춰 in_features만 맞춰주면 문제없이 작동한다.\n",
    " - out_features를 2이상으로 받아서 출력값을 matrix로 만들 수 있다.\n",
    " - layer를 여러개 사용한다면 출력시킨 matrix를 다시 입력값으로 받아 사용하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601747bf",
   "metadata": {},
   "source": [
    "### nn.Parameter\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
    "nn.Parameter는 단순한 값인데 tensor형식 데이터와 편미분 가능여부만 입력받는다.\n",
    "\n",
    "딥러닝에 필요한 값은 데이터와 가중치(Parameter)만 있으면 되는데 데이터는 고정된 값이고 가중치도 초기값은 어떤 값이든 크게 상관이 없는데 데이터의 입력에 맞춰서 차원(in_features, out_features)은 맞춰줘야 한다.\n",
    "\n",
    "실제로 딥러닝을 할 때는 위와 같이 parameter를 일일이 지정하지 않는데 이유는\n",
    "원하는 activation function을 선택하면 자동으로 생성해주기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2a09898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fee5c36c2e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ae1518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-5.4082e-01, -1.1962e+00, -5.0960e-01, -9.0026e-01, -1.3406e+00,\n",
      "         -9.9760e-01, -9.7645e-01, -1.0063e+00,  2.4372e-02,  4.6060e-01],\n",
      "        [-1.4423e-02,  7.0244e-01, -3.0078e-01,  1.8730e-01, -1.2472e+00,\n",
      "          1.1956e+00,  3.2985e-01, -1.1351e+00, -4.1356e-04, -1.0742e-02],\n",
      "        [-1.5019e+00,  8.0876e-01, -4.1119e-01, -5.3448e-01, -4.7063e-01,\n",
      "         -1.2862e+00, -5.7783e-01,  7.6014e-01,  8.9534e-01, -5.1637e-01],\n",
      "        [ 3.6535e-01, -5.8824e-01,  8.2542e-01, -3.8714e-01,  2.0533e+00,\n",
      "         -8.7402e-01,  4.1486e-02, -4.4455e-01,  6.4100e-01, -2.1823e-01],\n",
      "        [ 1.5629e+00,  1.0084e+00, -5.7811e-01,  1.5740e+00,  1.7311e-02,\n",
      "          9.6998e-02,  3.0074e+00,  5.8860e-01,  8.3850e-01, -1.2196e-01],\n",
      "        [ 1.4090e+00, -9.4593e-03, -7.3524e-01,  1.7171e+00, -1.0059e+00,\n",
      "          1.4688e-01,  1.9692e+00,  1.4780e+00, -2.3567e-01,  8.7229e-02],\n",
      "        [ 5.6492e-02,  3.8033e-01,  9.1157e-01,  1.8487e-01, -8.1197e-01,\n",
      "         -1.2960e+00,  7.3199e-02,  9.0382e-01, -2.6378e+00, -1.1993e+00]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3992,  0.8365, -0.7626, -0.2056, -0.1448,  2.5399, -0.4115,  1.7064,\n",
      "        -1.3589, -0.3227], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in layer.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92aa09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26f7bd",
   "metadata": {},
   "source": [
    "# AutoGrad for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cbeed",
   "metadata": {},
   "source": [
    "Pytorch에서 주어지는 함수들을 가지고 선형회귀를 위한 딥러닝을 해보자.\n",
    "https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c5aa4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data for training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train = torch.tensor(x_train, requires_grad=True)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train = torch.tensor(y_train, requires_grad=True)\n",
    "\n",
    "variables_count_with_x = x_train.shape[1]\n",
    "variables_count_with_y = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4e2313ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables count with x : 1\n",
      "variables count with y : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"variables count with x : {}\\nvariables count with y : {}\".format(variables_count_with_x,variables_count_with_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b5878ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Module\n",
    "\n",
    "class linearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(linearRegression, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f8514",
   "metadata": {},
   "source": [
    "activation function으로 Linear를 사용\n",
    "현재 layer가 단층이라 forward에 리턴값 하나만 있는데 다층구조가 되면 forawrd 함수 내부에서 출력값을 다시 입력값으로 쓰는 작업을 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d5aabb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = variables_count_with_x        # takes variable 'x' \n",
    "outputDim = variables_count_with_y       # takes variable 'y'\n",
    "\n",
    "# hypermarameters\n",
    "learningRate = 0.01 \n",
    "epochs = 10\n",
    "\n",
    "# create model\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fe7bf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5661]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5689], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747798a",
   "metadata": {},
   "source": [
    "### Backward\n",
    "\n",
    "forwawrdpropagation 다음은 backwardpropagation이다.\n",
    "\n",
    "backpropagation에서 필요한 계산은 \n",
    "\n",
    "**loss function을 사용해서 loss value를 구하기\n",
    "loss value의 optimizing을 위해 optimizer를 사용해서 wight의 편미분을 계산 후 wight를 업데이트**\n",
    "\n",
    "Pytorch로 구현해보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d34aad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e2541",
   "metadata": {},
   "source": [
    "**loss function은 MSE로 선택**\n",
    "loss function에 대한 자세한 내용은\n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "에서 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d6a295e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01508bff",
   "metadata": {},
   "source": [
    "**optimizer는 SGD로 선택**\n",
    "자세한 내용은\n",
    "https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "에서 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a0d9af7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(310.0353, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 310.0352783203125\n",
      "tensor(25.4676, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 25.467626571655273\n",
      "tensor(2.2544, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 2.254356622695923\n",
      "tensor(0.3589, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 0.3589472472667694\n",
      "tensor(0.2024, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 0.2023903876543045\n",
      "tensor(0.1877, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 0.18768729269504547\n",
      "tensor(0.1846, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 0.18457625806331635\n",
      "tensor(0.1824, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 0.18243266642093658\n",
      "tensor(0.1804, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 0.1803886592388153\n",
      "tensor(0.1784, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 0.1783735752105713\n"
     ]
    }
   ],
   "source": [
    "# Converting inputs and labels to Variable\n",
    "if torch.cuda.is_available():\n",
    "    inputs = x_train.cuda()\n",
    "    labels = y_train.cuda()\n",
    "else:\n",
    "    inputs = x_train\n",
    "    labels = y_train\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters(https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step)\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac501be9",
   "metadata": {},
   "source": [
    "모든 절차가 다 이해되는데 zero_grad 메소드가 이해가 안돼서 생각을 많이 했다.\n",
    "\n",
    "output layer에서 hidden layer를 거쳐 input layer로 가중치를 수정하는 작업을 할 때 가중치의 편미분 값이 필요한데 깊이가 깊을수록 함수의 합성이 많아져서 편미분의 합성연산이 많이 발생하고 결국 input layer에서는 모든 가중치에 대한 편미분 값을 알아야 input layer의 가중치를 업데이트 할 수 있는데 backward가 발생하면 optimizer는 이런 gradient 정보를 저장하고 있는다. 1 epoch의 연산을 마쳐도 optimizer에 전에 사용됐던 gradient가 남아있는데 이걸 모두 0으로 만들어주는 메소드가 zero_grad이다. 그래서 optimizer를 초기화 해줘야 정상적인 연산이 가능하다.\n",
    "\n",
    "당연히 초기화를 하는거라고 생각해서 step 메소드가 실행되면 자동으로 grad도 초기화 될 줄 알았는데 아니었다.  \n",
    "-> step에서 초기화를 해줘도 결국 시작시점에서 초기화를 하지 않으면 온전한 backpropagation을 할 수 없다. while문의 초기화 변수와 비슷한 논리  \n",
    "\n",
    "결국 zero_grad는 절차에 필요한 메소드임을 이해했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e433d80f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21435805]\n",
      " [ 2.3275084 ]\n",
      " [ 4.4406586 ]\n",
      " [ 6.5538087 ]\n",
      " [ 8.66696   ]\n",
      " [10.78011   ]\n",
      " [12.89326   ]\n",
      " [15.006411  ]\n",
      " [17.11956   ]\n",
      " [19.23271   ]\n",
      " [21.345861  ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(x_train).cuda().cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(x_train).data.numpy()\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7c3af469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 3.],\n",
       "        [ 5.],\n",
       "        [ 7.],\n",
       "        [ 9.],\n",
       "        [11.],\n",
       "        [13.],\n",
       "        [15.],\n",
       "        [17.],\n",
       "        [19.],\n",
       "        [21.]], requires_grad=True)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd889e",
   "metadata": {},
   "source": [
    "# Implementing a Logistic Regression Model from Scratch with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28a1c8",
   "metadata": {},
   "source": [
    "https://medium.com/dair-ai/implementing-a-logistic-regression-model-from-scratch-with-pytorch-24ea062cd856"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb1fec",
   "metadata": {},
   "source": [
    "다른 진도 나가고 시간나면 해보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
