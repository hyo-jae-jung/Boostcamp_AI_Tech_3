1. Problems with deeper layers

- layer를 깊게 쌓을 수 있으면 더 큰 receptive fields를 처리 할 수 있음. 하지만 쉽지 않았음

Gradient vanishing / exploding
Computationally complex
Degradation problem

이런 문제가 생김

- GoogLeNet
    깊이보다 병렬로 layer를 사용하는데 여기서 1x1 conv가 나옴.
    1x1 conv를 bottleneck 구조로 사용
    bottleneck은 병목현상인데 1x1 conv로 파라미터를 줄이는 게 병목현상과 비슷해서 bottleneck

    Auxiliary classifiers : Vanishing Gradient를 제거하기 위한 장치
        train 때만 사용하고 test에서는 비활성화

- ResNet
    100이 넘는 깊이를 구현
    인간보다 뛰어난 정확도
    결론은 depth가 중요한데 깊이 쌓을 수 있는 방법을 제시함 -> Residual block
    Residual block을 이용해서 input data가 잔차만큼 학습을 하도록 도와줌.
    H(x) = F(x) + x
    output data = residual + input data

    이게 Shortcut connection or Skip connection

    He initialization : output이 커지는 걸 방지하기 위해 initializing

- DenseNet
    ResNet에서 확장. skip connection을 layer 전후가 아니라 모든 layer 사이사이에 입력하는데
    sum이 아니라 concatenated로
    정보는 유지하지만 메모리는 많이 먹음

- SENet
    channel을 기준으로 1x1xC avgpool을 해서 각 채널의 평균값으로 벡터를 만들고(Squeeze)
    채널간의 연관성을 고려한 fc layer를 통과시켜서 채널별로 하나씩 있는 attention score를 생성(Excitation)
    입력 attention과 W(wight)를 활용해서 rescale을 진행

    영향력이 작은 채널은 줄이겠다는 의미. 영향력을 반영한 텐서가 만들어진다.

- EfficientNet
    Building deep, wide, and high resolution networks in as efficient way -> compound scaling
    효과를 봤던 방법들을 모두 적용. 효과가 나타나는 부분들이 다른데 그걸 조절

- Beyond ResNets
    


2. CNN architectures for image classification 2
3. Summary of image classification

